{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "438bb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import random\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from d2l import torch as d2l\n",
    "global rand_num\n",
    "global extracted_grads\n",
    "\n",
    "extracted_grads = []\n",
    "position = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01485fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_dir,is_train):\n",
    "    data,labels = [],[]\n",
    "    for label in ('neg','pos'):\n",
    "        data_path = os.path.join(data_dir,'train' if is_train else 'test',label)\n",
    "        for file in os.listdir(data_path):\n",
    "            with open (os.path.join(data_path,file),'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n',' ')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data,labels\n",
    "\n",
    "def read_test_data(data_dir,is_train):\n",
    "    data,labels = [],[]\n",
    "    label = 'pos'#Choose a label to attack\n",
    "    data_path = os.path.join(data_dir,'train' if is_train else 'test',label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open (os.path.join(data_path,file),'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n',' ')\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data,labels\n",
    "\n",
    "def preprocess(text):\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?)(') and prev_char != ' '\n",
    "    T = []\n",
    "    out = []\n",
    "    for i,t in enumerate(text):\n",
    "        lower = t.replace('<br />',' ').lower()\n",
    "        T.append(lower)   \n",
    "        out.append(''.join([' ' + char if j > 0 and no_space(char, T[i][j-1]) else char\n",
    "               for j, char in enumerate(T[i])]))\n",
    "    return out\n",
    "\n",
    "def tokenize(lines, token='word'):  \n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('Error: Unknown token type:' + token)\n",
    "\n",
    "class vocabulary:\n",
    "    def __init__(self,tokens = None,min_freg = 0,reserved_token = None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_token is None:\n",
    "            reserved_token = []\n",
    "        count = corpus_count(tokens)\n",
    "        self.token_fre = sorted(count.items(),key = lambda x:x[1],reverse = True)\n",
    "        \n",
    "        self.unk,unique_token = 0,['<unk>'] + reserved_token\n",
    "        unique_token += [token for token,fre in self.token_fre\n",
    "                         if fre >= min_freg and token not in unique_token]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in unique_token:\n",
    "            self.idx_to_token.append(token) \n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "    def __getitem__(self,tokens):\n",
    "        if not isinstance(tokens,(list,tuple)):\n",
    "            return self.token_to_idx.get(tokens,self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    def to_tokens(self,indexes):\n",
    "        if not isinstance(indexes,(list,tuple)):\n",
    "            return self.idx_to_token[indexes]\n",
    "        return [self.to_tokens(index) for index in indexes]\n",
    "            \n",
    "def corpus_count(tokens):\n",
    "    if len(tokens) == 0 or isinstance(tokens[0],list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  \n",
    "    return line + [padding_token] * (num_steps - len(line))  \n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "def try_all_gpus():\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "             for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "def init_weights_cnn(m):\n",
    "    if type(m) in (nn.Linear, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def load_imdb_data(batch_size, num_steps=1000):\n",
    "    data_dir = 'aclImdb'\n",
    "    train_data = read_data(data_dir,True)\n",
    "    test_data = read_test_data(data_dir,False)\n",
    "    train_tokens_pre = preprocess(train_data[0])\n",
    "    test_tokens_pre = preprocess(test_data[0])\n",
    "    train_tokens = tokenize(train_tokens_pre)\n",
    "    test_tokens = tokenize(test_tokens_pre)\n",
    "    vocab = vocabulary(train_tokens, min_freg = 5,reserved_token = ['<pad>'])\n",
    "    train_features = torch.tensor([truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = torch.tensor([truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    train_iter = load_array((train_features, torch.tensor(train_data[1])),\n",
    "                                batch_size)\n",
    "    test_iter = load_array((test_features, torch.tensor(test_data[1])),\n",
    "                               batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter,train_features,torch.tensor(train_data[1]), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa2340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model with different pooling layer\n",
    "class Model_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n",
    "                 **kwargs):\n",
    "        super(Model_CNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)#Average-over-time pooling layer\n",
    "        #self.pool = nn.AdaptiveMaxPool1d(1)#Max-over-time pooling layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(embed_size, c, k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        encoding = torch.cat([\n",
    "            torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\n",
    "            for conv in self.convs], dim=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b377c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_embedding(net): #load pretrained 100-dimensional GloVe embeddings\n",
    "    glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "    embeds = glove_embedding[vocab.idx_to_token]\n",
    "    net.embedding.weight.data.copy_(embeds)\n",
    "    net.embedding.weight.requires_grad = True\n",
    "\n",
    "def train(net,train_iter,lr,num_epochs,device):\n",
    "    print('---------------------------start---------------------')\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    optimizer = torch.optim.AdamW(net.parameters(),lr=lr)\n",
    "    net = net.to(device[0])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        print(f' epoch {epoch+1}')\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        train_length = 0\n",
    "        for batch in tqdm(train_iter):\n",
    "            x, y = batch\n",
    "            x = x.to(device[0])\n",
    "            y = y.to(device[0])\n",
    "            logits = net(x)\n",
    "            l = loss(logits,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.sum().backward()\n",
    "            optimizer.step()\n",
    "            acc = (logits.argmax(dim=-1) == y.to(device[0])).float().mean()\n",
    "            train_losses.append(l.sum())\n",
    "            train_accs.append(acc)\n",
    "            train_length += len(y)\n",
    "        print(\"Learning rate for epoch %d：%f\" % (epoch+1,optimizer.param_groups[0]['lr']))\n",
    "        train_loss = sum(train_losses) / train_length\n",
    "        train_acc = sum(train_accs) / len(train_iter)\n",
    "        print(f\"[ Train | {epoch + 1:03d}/{num_epochs:03d} ] loss = {train_loss:.5f}   acc = {train_acc:.5f}\")\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "    print('Starting validation')\n",
    "    print('saving model with loss {:.3f}'.format(train_loss))\n",
    "    save_path = f'./model.pth'\n",
    "    torch.save(net.state_dict(),save_path)\n",
    "\n",
    "def generate_position(num_trigger_tokens):#Generate distributed positions\n",
    "    a = random.sample(range(0,1000),num_trigger_tokens)\n",
    "    for i in range(len(a)):\n",
    "        position.append(a[i])\n",
    "\n",
    "    \n",
    "\n",
    "def init_trigger_tokens(trigger,num_trigger_tokens):#Initialize trigger tokens\n",
    "    trigger_token_ids = [vocab[trigger] ]* num_trigger_tokens\n",
    "    trigger_token_tensor = torch.tensor(trigger_token_ids)\n",
    "    return trigger_token_tensor\n",
    "\n",
    "def evaluate(net,test_iter,trigger_token_tensor):#evaluate the accuracy of the model after concatenating the initial trigger token\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    m = deepcopy(trigger_token_tensor)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter):\n",
    "            x, y = batch\n",
    "            for i in range(len(position)):\n",
    "                n = m[i].unsqueeze(0).unsqueeze(0)\n",
    "                x = torch.cat((x[:,:position[i]],n.repeat_interleave(x.shape[0],dim = 0),x[:,position[i]:]),dim = 1)\n",
    "            logits = net(x.to(device[0]))\n",
    "            acc = (logits.argmax(dim=-1) == y.to(device[0])).float().mean()\n",
    "            valid_accs.append(acc)\n",
    "    valid_acc = sum(valid_accs)/len(test_iter)\n",
    "    return valid_acc\n",
    "\n",
    "def extract_grad_hook(net, grad_in, grad_out):#store the gradient in extracted_grads\n",
    "    extracted_grads.append(grad_out[0].mean(dim = 0))\n",
    "def add_hook(net):\n",
    "    for module in net.modules():\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                hook = module.register_backward_hook(extract_grad_hook)\n",
    "    return hook\n",
    "\n",
    "\n",
    "def get_gradient(net,test_iter,trigger_token_tensor):#Calculate the loss to get the gradient\n",
    "    i = 0\n",
    "    net.train()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    m = deepcopy(trigger_token_tensor)\n",
    "    optimizer = torch.optim.AdamW(net.parameters())\n",
    "    for batch in tqdm(test_iter):\n",
    "        x, y = batch\n",
    "        for i in range(len(position)):\n",
    "            n = m[i].unsqueeze(0).unsqueeze(0)\n",
    "            x = torch.cat((x[:,:position[i]],n.repeat_interleave(x.shape[0],dim = 0),x[:,position[i]:]),dim = 1)\n",
    "        x = x.to(device[0])\n",
    "        y = y.to(device[0])\n",
    "        l = loss(net(x),y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "    \n",
    "\n",
    "def process_gradient(length,num_trigger_tokens):#Process the gradient to get the average gradient\n",
    "    extracted_grads_copy = deepcopy(extracted_grads)\n",
    "    extracted_grads_copy[0] = extracted_grads_copy[0].cpu()\n",
    "    temp = extracted_grads_copy[0]\n",
    "    temp = temp.unsqueeze(0)\n",
    "    for i in range(1,length-1):\n",
    "        extracted_grads_copy[i] = extracted_grads_copy[i].cpu()\n",
    "        extracted_grads_copy[i] = extracted_grads_copy[i].unsqueeze(0)\n",
    "        temp = torch.cat((temp,extracted_grads_copy[i]),dim = 0)\n",
    "    temp = temp.mean(dim = 0)\n",
    "    average_grad = temp[position[0]]\n",
    "    average_grad = average_grad.unsqueeze(0)\n",
    "    for i in range(1,len(position)):\n",
    "        temp_squeeze = temp[position[i]].unsqueeze(0)\n",
    "        average_grad = torch.cat((average_grad,temp_squeeze),dim = 0)\n",
    "    return average_grad\n",
    "\n",
    "def hotflip_attack(averaged_grad, embedding_matrix,\n",
    "                    num_candidates=1,increase_loss=False):#Return candidates according to Equation 3\n",
    "    averaged_grad = averaged_grad.cpu()\n",
    "    embedding_matrix = embedding_matrix.cpu()\n",
    "    averaged_grad = averaged_grad.unsqueeze(0)\n",
    "    gradient_dot_embedding_matrix = torch.einsum(\"bij,kj->bik\",\n",
    "                                                 (averaged_grad, embedding_matrix))  #Equation 3\n",
    "    if not increase_loss:\n",
    "        gradient_dot_embedding_matrix *= -1 \n",
    "    if num_candidates > 1:\n",
    "        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix, num_candidates, dim=2)\n",
    "        return best_k_ids.detach().cpu().numpy()[0]#Return candidates\n",
    "    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)\n",
    "    return best_at_each_step[0].detach().cpu().numpy()\n",
    "\n",
    "def get_embedding_weight(net):\n",
    "    for module in net.modules():\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                weight =  module.weight\n",
    "    return weight\n",
    "\n",
    "#\n",
    "def select_best_candid(net,test_iter,candid_trigger,trigger_token,valid_acc):#Concatenate each candidate to each input to determine the final trigger token\n",
    "    trigger_token = trigger_token.unsqueeze(0)\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    for i in range(candid_trigger.shape[0]):\n",
    "        trigger_token_temp = deepcopy(trigger_token)\n",
    "        for j in range(candid_trigger.shape[1]):\n",
    "            trigger_token_temp[0,i] = candid_trigger[i,j]\n",
    "            valid_accs = []\n",
    "            for batch in tqdm(test_iter):\n",
    "                x, y = batch\n",
    "                for k in range(len(position)):\n",
    "                    n = trigger_token_temp[0,k].unsqueeze(0).unsqueeze(0)\n",
    "                    x = torch.cat((x[:,:position[k]],n.repeat_interleave(x.shape[0],dim = 0),x[:,position[k]:]),dim = 1)\n",
    "                logits = net(x.to(device[0]))\n",
    "                acc = (logits.argmax(dim=-1) == y.to(device[0])).float().mean()\n",
    "                valid_accs.append(acc)\n",
    "            temp = sum(valid_accs)/len(test_iter)\n",
    "            if temp < valid_acc:\n",
    "                valid_acc = temp \n",
    "                trigger_token[0,i] = candid_trigger[i,j]\n",
    "    return trigger_token[0],valid_acc#Return the final trigger token and the accuracy after the attack\n",
    "\n",
    "def collection_attack(net,test_iter,num_candidates,num_epoch,trigger = 'the',#Summarize each function\n",
    "                      num_trigger_tokens=3):\n",
    "    trigger_token_tensor = init_trigger_tokens(trigger,num_trigger_tokens)\n",
    "    generate_position(num_trigger_tokens)\n",
    "    print(f'Distributed concatenation location:{position}')\n",
    "    valid_acc = evaluate(net,test_iter,trigger_token_tensor)\n",
    "    print(f'Initial trigger tokens state：the accuracy {valid_acc:.5f}')\n",
    "    embedding_weight = get_embedding_weight(net)\n",
    "    for i in range(num_epoch):\n",
    "        torch.cuda.empty_cache()\n",
    "        extracted_grads.clear()\n",
    "        hook = add_hook(net)\n",
    "        get_gradient(net,test_iter,trigger_token_tensor)\n",
    "        hook.remove()\n",
    "        average_grad = process_gradient(len(test_iter),num_trigger_tokens)\n",
    "        hot_token = hotflip_attack(average_grad,embedding_weight,num_candidates,increase_loss = True)\n",
    "        hot_token_tensor = torch.from_numpy(hot_token)\n",
    "        #print(hot_token_tensor)\n",
    "        trigger_token_tensor,valid_acc = select_best_candid(net,test_iter,hot_token_tensor,trigger_token_tensor,valid_acc)\n",
    "        print(f'after {i+1} rounds of attacking\\ntriggers: {trigger_token_tensor} \\ntriggers tokens:{vocab.to_tokens(trigger_token_tensor.numpy().tolist())} \\nthe accuracy :{valid_acc:.5f} ')\n",
    "    return trigger_token_tensor,valid_acc#Return the final trigger tokens (trigger length) and the accuracy after the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c3dfe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing and loading\n",
    "batch_size = 256\n",
    "train_iter, test_iter,train_features,train_labels,vocab = load_imdb_data(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a52c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, kernel_sizes, nums_channels = 100, [3,4,5], [100,100,100]\n",
    "device = try_all_gpus()\n",
    "net = Model_CNN(len(vocab), embed_size, kernel_sizes, nums_channels)\n",
    "net.apply(init_weights_cnn)\n",
    "put_embedding(net)\n",
    "lr, num_epochs = 0.001, 5\n",
    "train(net,train_iter,lr,num_epochs,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "219a3276-dba7-4705-a825-1e184132b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy of the model on the test set when no trigger token is concatenated\n",
    "def evaluate_no(net,test_iter):\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter):\n",
    "            x, y = batch\n",
    "            logits = net(x.to(device[0]))\n",
    "            acc = (logits.argmax(dim=-1) == y.to(device[0])).float().mean()\n",
    "            valid_accs.append(acc)\n",
    "    valid_acc = sum(valid_accs)/len(test_iter)\n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c26f58-db8d-40d2-80ad-d162698c6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_no(net,test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eabef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = []\n",
    "num_candidates,num_epoch = 5,10\n",
    "trigger_token_tensor,valid_acc = collection_attack(net,test_iter,num_candidates,num_epoch,trigger='the',num_trigger_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246cc8a1-db72-46b9-9332-1726f3454fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sequence):\n",
    "    \"\"\"The model's prediction for an input\"\"\"\n",
    "    sequence = torch.tensor(vocab[sequence.split()], device=d2l.try_gpu())\n",
    "    data = net(sequence.reshape(1, -1))\n",
    "    label = torch.argmax(net(sequence.reshape(1, -1)), dim=1)\n",
    "    return data,'positive' if label == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2c4e2-a64e-4f0c-9b9f-3e701ed8a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Figure 1\n",
    "predict_sentiment(net,vocab,\"This movie was sadly under-promoted but proved to be truly exceptional. Entering the theatre I knew nothing about the film except that a friend wanted to see it.<br /><br />I was caught off guard with the high quality of the film. I couldn't image Ashton Kutcher in a serious role, but his performance truly exemplified his character. This movie is exceptional and deserves our monetary support, unlike so many other movies. It does not come lightly for me to recommend any movie, but in this case I highly recommend that everyone see it.<br /><br />This films is Truly Exceptional!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
